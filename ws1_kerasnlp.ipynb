{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KerasNLP Guide\n",
        "\n",
        "KerasNLP is a natural language processing library that supports users through their entire development cycle. Our workflows are built from modular components that have state-of-the-art preset weights and architectures when used out-of-the-box and are easily customizable when more control is needed. We emphasize in-graph computation for all workflows so that developers can expect easy productionization using the TensorFlow ecosystem.\n",
        "\n",
        "This library is an extension of the core Keras API; all high-level modules are `Layers` or `Models` that recieve that same level of polish as core Keras."
      ],
      "metadata": {
        "id": "jYJzVtWoS0yk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up\n",
        "\n",
        "Begin by installing and importing some necessary libraries, including: [KerasNLP](https://keras.io/keras_nlp/) for using various models and preprocessing components and [TensorFlow Datasets](https://www.tensorflow.org/datasets) to download one of the datasets we will be working with, the [AG News Dataset](https://www.tensorflow.org/datasets/catalog/ag_news_subset)."
      ],
      "metadata": {
        "id": "H-EiszQuTktV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t7J4fxfCgwz"
      },
      "outputs": [],
      "source": [
        "!pip install keras_nlp datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFpuZw-3CmZ0"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmJyJcudvDHl"
      },
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5QUck8FZjEH"
      },
      "outputs": [],
      "source": [
        "ds = tfds.load('ag_news_subset', as_supervised=True) # ds['train] and ds['test] are tuples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what the data looks like:"
      ],
      "metadata": {
        "id": "iL0_9acQUIuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryqxEemeyomH"
      },
      "outputs": [],
      "source": [
        "for text, label in ds['train']:\n",
        "  print(text.numpy())\n",
        "  print(label.numpy())\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYEme9ii9jf9"
      },
      "outputs": [],
      "source": [
        "# Batch the data\n",
        "train_ds = ds['train'].batch(16)\n",
        "test_ds = ds['test'].batch(16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzDkPP4NtV6U"
      },
      "source": [
        "## Inference with a pre-trained classifier\n",
        "\n",
        "Our first task is to try running a sample of text with a pre-trained [BERT classifier](https://arxiv.org/abs/1810.04805). BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzDQvuzoHzLG"
      },
      "outputs": [],
      "source": [
        "# Create a BERT classifier to fit your data\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(preset=\"bert_medium_en_uncased\",\n",
        "                                                         num_classes=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bra_3dO5d45y"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(classifier, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1NQIMRHMzf1"
      },
      "outputs": [],
      "source": [
        "classifier.predict([text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "see0IWDvKuVm"
      },
      "outputs": [],
      "source": [
        "np.argmax(classifier.predict([text]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try compiling the model and observing how it performs on the test dataset."
      ],
      "metadata": {
        "id": "a39X19XmWovS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7J_1vFrn2C2"
      },
      "outputs": [],
      "source": [
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a 23% accuracy, but let's remember this model hasn't been trained on this specific dataset. Let's continue exploring other levels of usage of KerasNLP to see how it performs on the dataset."
      ],
      "metadata": {
        "id": "DJFVQ8TGWtZK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D96arUYntlO"
      },
      "outputs": [],
      "source": [
        "classifier.evaluate(test_ds, return_dict=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHE9iG5EtaBM"
      },
      "source": [
        "## Fine tuning a pre-trained BERT backbone\n",
        "\n",
        "When labeled text specific to our task is available, fine-tuning a custom classifier can improve performance. If we want to predict what category an AG News title belongs to, it helps to fit it to the AG News dataset. IFor many tasks no relevant pretrained model will be available (e.g., categorizing customer reviews), so we can consider fine-tuning to be an option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5eqSperbi8x"
      },
      "outputs": [],
      "source": [
        "# Recall the BERT classifier we created a couple cell blocks ago\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the huge jump in accuracy. We went from 23% to almost 90%. This is in part due to fine tuning."
      ],
      "metadata": {
        "id": "e7z7misxXTyJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIWTKimLE9SF"
      },
      "outputs": [],
      "source": [
        "history = classifier.fit(train_ds,\n",
        "                         validation_data=test_ds,\n",
        "                         epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvG-pvMHQUMX"
      },
      "source": [
        "## Fine tuning with user-controlled preprocessing\n",
        "\n",
        "For some advanced training scenarios, users might prefer direct control over preprocessing. For large datasets, examples can be preprocessed in advance and saved to disk or preprocessed by a separate worker pool using tf.data.experimental.service. In other cases, custom preprocessing is needed to handle the inputs.\n",
        "\n",
        "Pass `preprocessor=None` to the constructor of a task `Model` to skip automatic preprocessing or pass a custom `BertPreprocessor` instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVtMT4b6l7ey"
      },
      "source": [
        "### Separate preprocessing from the same preset\n",
        "\n",
        "Each model architecture has a parallel __preprocessor__ `Layer` with its own `from_preset` constructor. Using the same __preset__ for this `Layer` will return the matching preprocessor as the task.\n",
        "\n",
        "In this workflow we train the model over three epochs using `tf.data.Dataset.cache()`, which computes the preprocessing once and caches the result before fitting begins.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okXV9UTBdBhT"
      },
      "outputs": [],
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_medium_en_uncased\",\n",
        "    sequence_length=512\n",
        ")\n",
        "\n",
        "train_ds_cached = (\n",
        "    train_ds.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_ds_cached = (\n",
        "    test_ds.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(preset=\"bert_medium_en_uncased\",\n",
        "                                                         preprocessor=None,\n",
        "                                                         num_classes=4)\n",
        "\n",
        "classifier.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "\n",
        "classifier.fit(\n",
        "    train_ds_cached,\n",
        "    validation_data=test_ds_cached,\n",
        "    epochs=3\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see above, apply some preprocessing has further increased our accuracy over the three epochs. We're now at 94% accuracy, and the loss has also decreased."
      ],
      "metadata": {
        "id": "iNtW0dtbbf-J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxhx2xbpl_D3"
      },
      "source": [
        "### Custom preprocessing\n",
        "\n",
        "In cases where custom preprocessing is required, we offer direct access to the `Tokenizer` class that maps raw strings to tokens. It also has a `from_preset()` constructor to get the vocabulary matching pretraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z16kL6W7mBs-"
      },
      "outputs": [],
      "source": [
        "# Tokenizer class maps raw strings to tokens\n",
        "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_medium_en_uncased\")\n",
        "tokenizer([\"I would like to learn natural language processing\",\n",
        "           \"Can you believe my improvement in training accuracy?\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lHOmG_Eszm1"
      },
      "outputs": [],
      "source": [
        "packer = keras_nlp.layers.MultiSegmentPacker(\n",
        "    start_value=tokenizer.cls_token_id,\n",
        "    end_value=tokenizer.sep_token_id,\n",
        "    sequence_length=64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtGrT913rl7U"
      },
      "outputs": [],
      "source": [
        "def preprocessor(x, y):\n",
        "  token_ids, segment_ids = packer(tokenizer(x))\n",
        "  x = {\n",
        "      \"token_ids\": token_ids,\n",
        "       \"segment_ids\": segment_ids,\n",
        "       \"padding_mask\": token_ids != 0\n",
        "  }\n",
        "  return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjphTlfVs0ql"
      },
      "outputs": [],
      "source": [
        "train_ds_preprocessed = (\n",
        "    train_ds.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_ds_preprocessed = (\n",
        "    test_ds.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDdP_-yGtYy4"
      },
      "outputs": [],
      "source": [
        "# Preprocessed example\n",
        "print(train_ds_preprocessed.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etEobHRJQZd-"
      },
      "source": [
        "## Fine tuning with a custom model\n",
        "\n",
        "For more advanced applications, an appropriate task `Model` may not be available. In this case we provide direct access to the backbone `Model`, which has its own `from_preset` constructor and can be composed with custom `Layer`s. This is a form of transfer learning.\n",
        "\n",
        "A backbone `Model` does not include automatic preprocessing but can be paired with a matching preprocessor using the same preset as shown in the previous workflow.\n",
        "\n",
        "In this workflow we experiment with freezing our backbone model and adding two trainable transfomer layers to adapt to the new input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRg69HszQc4m"
      },
      "outputs": [],
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_medium_en_uncased\")\n",
        "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_medium_en_uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMZ9tYMWuAFu"
      },
      "outputs": [],
      "source": [
        "train_ds_preprocessed = (\n",
        "    train_ds.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_ds_preprocessed = (\n",
        "    test_ds.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W737RonNuVeo"
      },
      "outputs": [],
      "source": [
        "backbone.trainable = False\n",
        "inputs = backbone.input\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "for _ in range(2):\n",
        "  sequence = keras_nlp.layers.TransformerEncoder(\n",
        "      num_heads=2,\n",
        "      intermediate_dim=512,\n",
        "      dropout=0.1\n",
        "  )(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfRTZj-gvT8W"
      },
      "outputs": [],
      "source": [
        "# Use [CLS] token output to classify\n",
        "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGlTjoj8usOC"
      },
      "outputs": [],
      "source": [
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate = 0.0001),\n",
        "    metrics=keras.metrics.SparseCategoricalAccuracy(),\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.summary()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}